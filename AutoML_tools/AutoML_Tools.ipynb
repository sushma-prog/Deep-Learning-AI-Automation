{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ **Step 1 ‚Äî What is AutoML?**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ First, the problem with traditional ML:\n",
        "\n",
        "When you build a **machine learning model manually**, you usually have to:\n",
        "\n",
        "1. **Clean data** (missing values, encoding, scaling).\n",
        "2. **Choose an algorithm** (Logistic Regression? Random Forest? XGBoost?).\n",
        "3. **Tune hyperparameters** (learning rate, depth, number of trees, etc.).\n",
        "4. **Train & evaluate models**.\n",
        "5. **Compare results** and pick the best one.\n",
        "\n",
        "üëâ This can take **hours/days** and needs **expert knowledge**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What AutoML does:\n",
        "\n",
        "**AutoML = Automated Machine Learning**\n",
        "\n",
        "It automates many steps of ML for you:\n",
        "\n",
        "* üßπ **Data preprocessing** ‚Üí handles missing values, encoding, scaling.\n",
        "* üß† **Model selection** ‚Üí tries multiple algorithms (Logistic Regression, GBM, XGBoost, Neural Nets).\n",
        "* üéöÔ∏è **Hyperparameter tuning** ‚Üí automatically adjusts parameters for better accuracy.\n",
        "* üìä **Model evaluation** ‚Üí gives you a leaderboard of the best models.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why it matters:\n",
        "\n",
        "1. **Saves time** ‚è± ‚Äî You don‚Äôt need to manually try 10 models. AutoML does it.\n",
        "2. **Easy for beginners** ‚Äî Even without deep ML knowledge, you can get strong results.\n",
        "3. **Great for prototyping** üöÄ ‚Äî In companies, quick experiments are more important than perfect models.\n",
        "4. **Benchmarking** ‚Äî You can compare your custom-built models vs AutoML‚Äôs best.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Real-world analogy:\n",
        "\n",
        "Think of **AutoML** like a **smart cooking robot** üë©‚Äçüç≥ü§ñ:\n",
        "\n",
        "* You give it raw ingredients (dataset).\n",
        "* It tries different recipes (algorithms).\n",
        "* It tweaks the seasoning (hyperparameters).\n",
        "* Finally, it serves you the best dish (the most accurate model).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example:\n",
        "\n",
        "If you give AutoML the **Titanic dataset**, it will automatically:\n",
        "\n",
        "* Handle missing ages in passengers.\n",
        "* Encode \"male/female\" into numbers.\n",
        "* Try Logistic Regression, Random Forest, XGBoost, etc.\n",
        "* Tune parameters like depth, learning rate.\n",
        "* Show you:\n",
        "\n",
        "  * \"XGBoost gave 82% accuracy\"\n",
        "  * \"Random Forest gave 80% accuracy\"\n",
        "  * \"Best model = XGBoost ‚úÖ\"\n",
        "\n",
        "---\n",
        "\n",
        "üëâ That‚Äôs why **AutoML is very powerful** ‚Äî it reduces weeks of work into hours (or even minutes).\n",
        "\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "Gw3nwFqFsI-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ **Step 2 ‚Äî Popular AutoML Tools**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. **Google AutoML**\n",
        "\n",
        "* ‚òÅÔ∏è **Cloud-based**: Runs on Google Cloud Platform (GCP).\n",
        "* ‚ö° **Strengths**:\n",
        "\n",
        "  * Extremely powerful for **large datasets**.\n",
        "  * Has pre-trained models for **vision, text, translation, video**.\n",
        "  * Google‚Äôs infrastructure ‚Üí scales easily.\n",
        "* üí∞ **Limitation**: Paid service (credits free initially, but costs later).\n",
        "* üåç **Real-world use**: Companies that already use Google Cloud.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **H2O.ai AutoML**\n",
        "\n",
        "* üñ•Ô∏è **Open-source**: Free, runs locally on laptop/Colab.\n",
        "* üß† **Strengths**:\n",
        "\n",
        "  * Tries multiple algorithms (GLM, GBM, XGBoost, Deep Learning).\n",
        "  * Does **stacked ensembles** (combines models for better accuracy).\n",
        "  * Gives a **leaderboard** so you can see best ‚Üí worst models.\n",
        "* üìä **Great for students & researchers**: Easy to test datasets like **Titanic**, **Iris**, **Breast Cancer**.\n",
        "* ‚ö° **Fast prototyping**: Build strong baselines before fine-tuning manually.\n",
        "\n",
        "üëâ This is the one we‚Äôll **practice with today** since it‚Äôs free + Python-friendly.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **Teachable Machine (by Google)**\n",
        "\n",
        "* üåê **Web-based (no coding needed)**.\n",
        "* ‚ö° **Strengths**:\n",
        "\n",
        "  * Best for **images, sounds, poses**.\n",
        "  * You just drag-drop your data ‚Üí model is ready in minutes.\n",
        "  * Great for **absolute beginners** or school projects.\n",
        "* üé® **Example**: Upload cat/dog images ‚Üí get a ready-to-use classifier.\n",
        "* ‚ùå **Limitation**: Not suitable for big datasets or advanced ML tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### üåü Summary:\n",
        "\n",
        "| Tool                  | Type        | Best For                       | Free/Paid           |\n",
        "| --------------------- | ----------- | ------------------------------ | ------------------- |\n",
        "| **Google AutoML**     | Cloud-based | Large-scale ML (vision, text)  | Paid (with credits) |\n",
        "| **H2O.ai AutoML**     | Open-source | Tabular datasets, Python users | Free ‚úÖ              |\n",
        "| **Teachable Machine** | Web tool    | Quick image/sound projects     | Free ‚úÖ              |\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Since our goal is **hands-on coding + portfolio projects**, we‚Äôll continue with **H2O.ai AutoML**.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "LadA6AMPz2QP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ **Step 3 ‚Äî Dataset for Practice**\n",
        "\n",
        "AutoML tools can work with **any dataset**, but for practice, it‚Äôs better to start with **small, classic datasets** that are:\n",
        "\n",
        "‚úÖ Easy to understand,\n",
        "\n",
        "‚úÖ Small enough to run quickly,\n",
        "\n",
        "‚úÖ Commonly used in ML tutorials (so you can compare your results).\n",
        "\n",
        "---\n",
        "\n",
        "### üõ≥ **Option 1: Titanic Survival Dataset**\n",
        "\n",
        "* üìä **What it‚Äôs about**: Passenger data from the Titanic disaster (1912).\n",
        "* üéØ **Goal**: Predict whether a passenger survived (`1`) or not (`0`).\n",
        "* üîë **Features include**:\n",
        "\n",
        "  * Passenger Class (`Pclass`)\n",
        "  * Gender (`Sex`)\n",
        "  * Age\n",
        "  * Number of siblings/spouses aboard (`SibSp`)\n",
        "  * Number of parents/children aboard (`Parch`)\n",
        "  * Ticket Fare\n",
        "* ü§î **Why it‚Äôs good**:\n",
        "\n",
        "  * Real-world dataset with **categorical + numerical features**.\n",
        "  * Teaches you how to handle **missing values** and **categorical encoding**.\n",
        "  * Great for classification practice.\n",
        "\n",
        "---\n",
        "\n",
        "### üß¨ **Option 2: Breast Cancer Dataset**\n",
        "\n",
        "* üìä **What it‚Äôs about**: Medical data on breast tumors.\n",
        "* üéØ **Goal**: Predict whether a tumor is **benign (non-cancerous)** or **malignant (cancerous)**.\n",
        "* üîë **Features include**:\n",
        "\n",
        "  * Tumor size, shape, smoothness, texture, etc.\n",
        "  * All values are **numerical**.\n",
        "* ü§î **Why it‚Äôs good**:\n",
        "\n",
        "  * Very clean dataset (no missing values).\n",
        "  * Quick to run and widely used in ML papers.\n",
        "  * Perfect for **binary classification**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Which one will we use?\n",
        "\n",
        "Since **H2O AutoML** works best with *structured tabular datasets*, we‚Äôll pick **Titanic Survival** first üö¢ because it includes both **numerical + categorical** features ‚Üí more variety.\n",
        "\n",
        "But we can also try **Breast Cancer** as a second quick run for comparison üß¨.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "HE0kmntk2AFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîπ Step 4 ‚Äî Install & Set Up H2O.ai**\n",
        "\n",
        "H2O.ai provides an AutoML engine that we can run in Python. To use it, we first need to install the library and then start the H2O engine (like starting a car before driving üöó)."
      ],
      "metadata": {
        "id": "yAht4C0-3bMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install H2O library (only needed once per environment)\n",
        "!pip install h2o\n",
        "\n",
        "# Import H2O Package\n",
        "import h2o\n",
        "\n",
        "# Initialize H2O engine\n",
        "h2o.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "gXRtIf54sXEP",
        "outputId": "e409b728-90cd-410b-8ccb-02c73450b696"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h2o in /usr/local/lib/python3.12/dist-packages (3.46.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from h2o) (2.32.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->h2o) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->h2o) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->h2o) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->h2o) (2025.8.3)\n",
            "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n",
            "Warning: Your H2O cluster version is (4 months and 25 days) old.  There may be a newer version available.\n",
            "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         6 mins 27 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.7\n",
              "H2O_cluster_version_age:    4 months and 25 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_cz2hz8\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.166 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://localhost:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.12.11 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-13.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-13 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-13 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-13 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-13 .h2o-table th,\n",
              "#h2o-table-13 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>6 mins 27 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.7</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>4 months and 25 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_cz2hz8</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.166 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://localhost:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.12.11 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# **üîπ Step 5 ‚Äî Run H2O AutoML**"
      ],
      "metadata": {
        "id": "LgEJNgxb7_Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "#h2o.automl ‚Üí The AutoML part of H2O library.\n",
        "#H2OAutoML ‚Üí The class that automatically builds, trains, and compares many ML models."
      ],
      "metadata": {
        "id": "SorywXpQ3uvf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Titanic dataset directly from H2O example repo\n",
        "titanic = h2o.import_file(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\")\n",
        "#h2o.import_file() ‚Üí H2O‚Äôs function to load data (similar to pandas read_csv).\n",
        "#\"https://.../titanic.csv\" ‚Üí A URL where the dataset is hosted."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxb-X4198SjY",
        "outputId": "190e51c4-7131-4c32-f2f3-6c594bf68aca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train/test (80/20 split)\n",
        "train, test = titanic.split_frame(ratios=[0.8], seed=42)"
      ],
      "metadata": {
        "id": "MrKybn_x4WxR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target column (what we want to predict)\n",
        "y = \"survived\"\n",
        "#\"survived\" ‚Üí Column in Titanic dataset ‚Üí 0 = did not survive, 1 = survived.\n",
        "titanic[y] = titanic[y].asfactor() # Convert target to categorical (important!)\n",
        "\n",
        "titanic = titanic.drop(\"alive\") # Normally we must drop alive before training. alive is basically the target column (survived). model may ‚Äúcheat‚Äù by using the label itself."
      ],
      "metadata": {
        "id": "ty1RZzbt8q5I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature columns (all except target)\n",
        "x = [col for col in titanic.columns if col != y]\n",
        "#x ‚Üí List of features/independent variables.\n",
        "#titanic.columns ‚Üí Gets all column names.\n",
        "#if col != y ‚Üí Excludes the target column (survived).\n",
        "#So x = all features like age, class, sex, etc."
      ],
      "metadata": {
        "id": "KHaEiHPU8ysx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run AutoML for 20 seconds\n",
        "aml = H2OAutoML(max_runtime_secs=20, seed=42)\n",
        "#H2OAutoML(...) ‚Üí Creates an AutoML object.\n",
        "#max_runtime_secs=20 ‚Üí Runs for 20 seconds (time budget).\n",
        "#seed=42 ‚Üí Random seed for reproducibility.\n",
        "\n",
        "aml.train(x=x, y=y, training_frame=titanic)\n",
        "#.train() ‚Üí Starts AutoML process:\n",
        "#x=x ‚Üí Feature columns.\n",
        "#y=y ‚Üí Target column.\n",
        "#training_frame=titanic ‚Üí Dataset to train on.\n",
        "\n",
        "#H2OAutoML will now try many models: GBM, Random Forest, Deep Learning, Stacked Ensembles, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nmfUSE99G8wt",
        "outputId": "69240907-6d56-43de-e7ab-f681f1c8452f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (done) 100%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OGradientBoostingEstimator : Gradient Boosting Machine\n",
              "Model Key: GBM_3_AutoML_2_20250822_141659\n",
              "\n",
              "\n",
              "Model Summary: \n",
              "    number_of_trees    number_of_internal_trees    model_size_in_bytes    min_depth    max_depth    mean_depth    min_leaves    max_leaves    mean_leaves\n",
              "--  -----------------  --------------------------  ---------------------  -----------  -----------  ------------  ------------  ------------  -------------\n",
              "    33                 33                          17377                  8            8            8             26            48            37.2121\n",
              "\n",
              "ModelMetricsBinomial: gbm\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.07952647790733995\n",
              "RMSE: 0.2820043934185068\n",
              "LogLoss: 0.2757158691230326\n",
              "Mean Per-Class Error: 0.11156648451730419\n",
              "AUC: 0.9563800210909789\n",
              "AUCPR: 0.9448231652163201\n",
              "Gini: 0.9127600421819577\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4414577036917207\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  ------------\n",
              "0      518  31   0.0565   (31.0/549.0)\n",
              "1      57   285  0.1667   (57.0/342.0)\n",
              "Total  575  316  0.0988   (88.0/891.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.441458     0.866261  178\n",
              "max f2                       0.228137     0.89415   249\n",
              "max f0point5                 0.6345       0.901288  131\n",
              "max accuracy                 0.484042     0.902357  169\n",
              "max precision                0.978619     1         0\n",
              "max recall                   0.0742427    1         357\n",
              "max specificity              0.978619     1         0\n",
              "max absolute_mcc             0.484042     0.792463  169\n",
              "max min_per_class_accuracy   0.320907     0.883424  212\n",
              "max mean_per_class_accuracy  0.334525     0.888505  205\n",
              "max tns                      0.978619     549       0\n",
              "max fns                      0.978619     341       0\n",
              "max fps                      0.02919      549       399\n",
              "max tps                      0.0742427    342       357\n",
              "max tnr                      0.978619     1         0\n",
              "max fnr                      0.978619     0.997076  0\n",
              "max fpr                      0.02919      1         399\n",
              "max tpr                      0.0742427    1         357\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 38.38 %, avg score: 38.40 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.010101                    0.974971           2.60526    2.60526            1                0.976646   1                           0.976646            0.0263158       0.0263158                  160.526   160.526            0.0263158\n",
              "2        0.020202                    0.971498           2.60526    2.60526            1                0.973037   1                           0.974841            0.0263158       0.0526316                  160.526   160.526            0.0526316\n",
              "3        0.030303                    0.969515           2.60526    2.60526            1                0.970587   1                           0.973423            0.0263158       0.0789474                  160.526   160.526            0.0789474\n",
              "4        0.040404                    0.96779            2.60526    2.60526            1                0.968647   1                           0.972229            0.0263158       0.105263                   160.526   160.526            0.105263\n",
              "5        0.0505051                   0.96639            2.60526    2.60526            1                0.967099   1                           0.971203            0.0263158       0.131579                   160.526   160.526            0.131579\n",
              "6        0.10101                     0.951705           2.60526    2.60526            1                0.959634   1                           0.965419            0.131579        0.263158                   160.526   160.526            0.263158\n",
              "7        0.150393                    0.929738           2.60526    2.60526            1                0.941504   1                           0.957566            0.128655        0.391813                   160.526   160.526            0.391813\n",
              "8        0.200898                    0.851433           2.60526    2.60526            1                0.88939    1                           0.940427            0.131579        0.523392                   160.526   160.526            0.523392\n",
              "9        0.300786                    0.629927           2.19545    2.46917            0.842697         0.741349   0.947761                    0.874315            0.219298        0.74269                    119.545   146.917            0.717189\n",
              "10       0.402918                    0.331198           1.34558    2.18436            0.516484         0.459742   0.83844                     0.769228            0.137427        0.880117                   34.5575   118.436            0.77447\n",
              "11       0.500561                    0.202457           0.598911   1.87509            0.229885         0.259296   0.719731                    0.669757            0.0584795       0.938596                   -40.1089  87.5089            0.71091\n",
              "12       0.600449                    0.135548           0.351271   1.62159            0.134831         0.16514    0.62243                     0.585812            0.0350877       0.973684                   -64.8729  62.1594            0.605742\n",
              "13       0.700337                    0.0938176          0.146363   1.41118            0.0561798        0.112169   0.541667                    0.518257            0.0146199       0.988304                   -85.3637  41.1184            0.467357\n",
              "14       0.800224                    0.0781551          0.0878179  1.246              0.0337079        0.0868113  0.478261                    0.464402            0.00877193      0.997076                   -91.2182  24.5995            0.31948\n",
              "15       0.900112                    0.0648662          0.0292726  1.11097            0.011236         0.0723062  0.426434                    0.42089             0.00292398      1                          -97.0727  11.0973            0.162113\n",
              "16       1                           0.02919            0          1                  0                0.0516458  0.383838                    0.384007            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomial: gbm\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.12564572227434634\n",
              "RMSE: 0.35446540349425687\n",
              "LogLoss: 0.4067842408627829\n",
              "Mean Per-Class Error: 0.1848816029143898\n",
              "AUC: 0.8754673569168824\n",
              "AUCPR: 0.8577731956886376\n",
              "Gini: 0.7509347138337648\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4002474396191816\n",
              "       0    1    Error    Rate\n",
              "-----  ---  ---  -------  -------------\n",
              "0      468  81   0.1475   (81.0/549.0)\n",
              "1      76   266  0.2222   (76.0/342.0)\n",
              "Total  544  347  0.1762   (157.0/891.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.400247     0.772134  194\n",
              "max f2                       0.186723     0.820569  264\n",
              "max f0point5                 0.640277     0.82106   128\n",
              "max accuracy                 0.554883     0.839506  146\n",
              "max precision                0.996056     1         0\n",
              "max recall                   0.0196822    1         397\n",
              "max specificity              0.996056     1         0\n",
              "max absolute_mcc             0.554883     0.656544  146\n",
              "max min_per_class_accuracy   0.345827     0.804094  213\n",
              "max mean_per_class_accuracy  0.400247     0.815118  194\n",
              "max tns                      0.996056     549       0\n",
              "max fns                      0.996056     341       0\n",
              "max fps                      0.016024     549       399\n",
              "max tps                      0.0196822    342       397\n",
              "max tnr                      0.996056     1         0\n",
              "max fnr                      0.996056     0.997076  0\n",
              "max fpr                      0.016024     1         399\n",
              "max tpr                      0.0196822    1         397\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 38.38 %, avg score: 38.23 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.010101                    0.97968            2.60526   2.60526            1                0.986904   1                           0.986904            0.0263158       0.0263158                  160.526   160.526            0.0263158\n",
              "2        0.020202                    0.968141           2.60526   2.60526            1                0.973158   1                           0.980031            0.0263158       0.0526316                  160.526   160.526            0.0526316\n",
              "3        0.030303                    0.96295            2.60526   2.60526            1                0.966069   1                           0.975377            0.0263158       0.0789474                  160.526   160.526            0.0789474\n",
              "4        0.040404                    0.960372           2.60526   2.60526            1                0.961511   1                           0.97191             0.0263158       0.105263                   160.526   160.526            0.105263\n",
              "5        0.0505051                   0.95714            2.60526   2.60526            1                0.959482   1                           0.969425            0.0263158       0.131579                   160.526   160.526            0.131579\n",
              "6        0.10101                     0.931532           2.48947   2.54737            0.955556         0.944153   0.977778                    0.956789            0.125731        0.25731                    148.947   154.737            0.253667\n",
              "7        0.150393                    0.902304           2.25      2.44973            0.863636         0.915342   0.940299                    0.943179            0.111111        0.368421                   125       144.973            0.353849\n",
              "8        0.20202                     0.812577           2.49199   2.46053            0.956522         0.864452   0.944444                    0.92306             0.128655        0.497076                   149.199   146.053            0.478861\n",
              "9        0.300786                    0.59327            1.80592   2.24558            0.693182         0.707623   0.86194                     0.85232             0.178363        0.675439                   80.5921   124.558            0.608043\n",
              "10       0.400673                    0.381019           1.08309   1.95577            0.41573          0.473092   0.7507                      0.757778            0.108187        0.783626                   8.30869   95.5772            0.621513\n",
              "11       0.500561                    0.199447           0.819633  1.72905            0.314607         0.287965   0.663677                    0.664026            0.0818713       0.865497                   -18.0367  72.9054            0.592273\n",
              "12       0.600449                    0.133415           0.351271  1.49985            0.134831         0.164221   0.575701                    0.580881            0.0350877       0.900585                   -64.8729  49.9852            0.487106\n",
              "13       0.700337                    0.106235           0.234181  1.31933            0.0898876        0.119834   0.50641                     0.515123            0.0233918       0.923977                   -76.5819  31.9332            0.362957\n",
              "14       0.800224                    0.0852813          0.175636  1.17657            0.0674157        0.0947699  0.451613                    0.462652            0.0175439       0.94152                    -82.4364  17.657             0.229316\n",
              "15       0.900112                    0.0616176          0.292726  1.07849            0.11236          0.0731676  0.413965                    0.41943             0.0292398       0.97076                    -70.7274  7.8488             0.114658\n",
              "16       1                           0.0155573          0.292726  1                  0.11236          0.0474298  0.383838                    0.382272            0.0292398       1                          -70.7274  0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                         mean        sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
              "-----------------------  ----------  -----------  ------------  ------------  ------------  ------------  ------------\n",
              "accuracy                 0.84397715  0.041625928  0.8603352     0.8651685     0.78089887    0.8876405     0.8258427\n",
              "aic                      nan         0.0          nan           nan           nan           nan           nan\n",
              "auc                      0.8687463   0.03669451   0.88440645    0.86333984    0.81169784    0.91166395    0.8726233\n",
              "err                      0.15602285  0.041625928  0.1396648     0.13483146    0.21910113    0.11235955    0.1741573\n",
              "err_count                27.8        7.395945     25.0          24.0          39.0          20.0          31.0\n",
              "f0point5                 0.8088298   0.06946605   0.81395346    0.8737864     0.71428573    0.875         0.7671233\n",
              "f1                       0.7921073   0.048578985  0.81751823    0.8181818     0.7111111     0.8305085     0.7832168\n",
              "f2                       0.7777265   0.043232445  0.82111436    0.7692308     0.7079646     0.7903226     0.8\n",
              "lift_top_group           2.6098633   0.12274331   2.6323528     2.4383562     2.6176472     2.78125       2.5797102\n",
              "loglikelihood            nan         0.0          nan           nan           nan           nan           nan\n",
              "---                      ---         ---          ---           ---           ---           ---           ---\n",
              "mcc                      0.670955    0.086933136  0.7044551     0.7231524     0.5346942     0.75340885    0.63906443\n",
              "mean_per_class_accuracy  0.8299896   0.038138706  0.85320616    0.8460535     0.76657754    0.8608827     0.8232283\n",
              "mean_per_class_error     0.17001037  0.038138706  0.14679386    0.1539465     0.23342246    0.13911733    0.1767717\n",
              "mse                      0.12741663  0.026073605  0.1165862     0.118222356   0.16973715    0.10081037    0.13172702\n",
              "pr_auc                   0.84940624  0.058225177  0.87730867    0.87913716    0.7500293     0.8942694     0.84628665\n",
              "precision                0.8214861   0.08874401   0.8115942     0.91525424    0.7164179     0.9074074     0.7567568\n",
              "r2                       0.4609438   0.108819194  0.5050956     0.5113168     0.2810225     0.5622155     0.4450686\n",
              "recall                   0.7692714   0.0490906    0.8235294     0.739726      0.7058824     0.765625      0.8115942\n",
              "rmse                     0.35554448  0.035439212  0.3414472     0.3438348     0.4119917     0.3175065     0.3629422\n",
              "specificity              0.89070785  0.061824236  0.8828829     0.95238096    0.8272727     0.95614034    0.8348624\n",
              "[22 rows x 8 columns]\n",
              "\n",
              "\n",
              "Scoring History: \n",
              "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error\n",
              "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------\n",
              "    2025-08-22 14:17:13  1.372 sec   0                  0.486319         0.665912            0.5             0.383838           1                0.616162\n",
              "    2025-08-22 14:17:13  1.399 sec   5                  0.389724         0.485661            0.927063        0.909573           2.60526          0.129068\n",
              "    2025-08-22 14:17:13  1.422 sec   10                 0.344525         0.403489            0.933851        0.917227           2.60526          0.126824\n",
              "    2025-08-22 14:17:13  1.447 sec   15                 0.318329         0.352145            0.94249         0.926531           2.60526          0.111111\n",
              "    2025-08-22 14:17:13  1.470 sec   20                 0.305364         0.32428             0.945952        0.931625           2.60526          0.106622\n",
              "    2025-08-22 14:17:13  1.489 sec   25                 0.295265         0.302767            0.949304        0.936386           2.60526          0.103255\n",
              "    2025-08-22 14:17:13  1.507 sec   30                 0.286253         0.284529            0.95348         0.941646           2.60526          0.10101\n",
              "    2025-08-22 14:17:13  1.523 sec   33                 0.282004         0.275716            0.95638         0.944823           2.60526          0.0987654\n",
              "\n",
              "Variable Importances: \n",
              "variable     relative_importance    scaled_importance    percentage\n",
              "-----------  ---------------------  -------------------  ------------\n",
              "adult_male   231.292                1                    0.320339\n",
              "fare         101.336                0.43813              0.14035\n",
              "age          91.8512                0.397122             0.127214\n",
              "pclass       71.0012                0.306976             0.0983364\n",
              "who          62.5804                0.270569             0.0866737\n",
              "sex          50.1718                0.21692              0.0694878\n",
              "deck         34.4982                0.149154             0.0477799\n",
              "class        23.2359                0.100461             0.0321816\n",
              "sibsp        21.0053                0.0908173            0.0290923\n",
              "parch        12.5569                0.0542901            0.0173912\n",
              "embark_town  10.2541                0.0443339            0.0142019\n",
              "embarked     7.09572                0.0306786            0.00982755\n",
              "alone        5.14414                0.0222409            0.00712462\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ],
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OGradientBoostingEstimator : Gradient Boosting Machine\n",
              "Model Key: GBM_3_AutoML_2_20250822_141659\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-14.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-14 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-14 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-14 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table th,\n",
              "#h2o-table-14 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>number_of_trees</th>\n",
              "<th>number_of_internal_trees</th>\n",
              "<th>model_size_in_bytes</th>\n",
              "<th>min_depth</th>\n",
              "<th>max_depth</th>\n",
              "<th>mean_depth</th>\n",
              "<th>min_leaves</th>\n",
              "<th>max_leaves</th>\n",
              "<th>mean_leaves</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>33.0</td>\n",
              "<td>33.0</td>\n",
              "<td>17377.0</td>\n",
              "<td>8.0</td>\n",
              "<td>8.0</td>\n",
              "<td>8.0</td>\n",
              "<td>26.0</td>\n",
              "<td>48.0</td>\n",
              "<td>37.21212</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: gbm\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.07952647790733995\n",
              "RMSE: 0.2820043934185068\n",
              "LogLoss: 0.2757158691230326\n",
              "Mean Per-Class Error: 0.11156648451730419\n",
              "AUC: 0.9563800210909789\n",
              "AUCPR: 0.9448231652163201\n",
              "Gini: 0.9127600421819577</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-15.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-15 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-15 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-15 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-15 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-15 .h2o-table th,\n",
              "#h2o-table-15 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-15 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-15\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4414577036917207</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>518.0</td>\n",
              "<td>31.0</td>\n",
              "<td>0.0565</td>\n",
              "<td> (31.0/549.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>57.0</td>\n",
              "<td>285.0</td>\n",
              "<td>0.1667</td>\n",
              "<td> (57.0/342.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>575.0</td>\n",
              "<td>316.0</td>\n",
              "<td>0.0988</td>\n",
              "<td> (88.0/891.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-16.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-16 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-16 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-16 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-16 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-16 .h2o-table th,\n",
              "#h2o-table-16 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-16 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-16\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4414577</td>\n",
              "<td>0.8662614</td>\n",
              "<td>178.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.2281365</td>\n",
              "<td>0.8941504</td>\n",
              "<td>249.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6344999</td>\n",
              "<td>0.9012876</td>\n",
              "<td>131.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.4840417</td>\n",
              "<td>0.9023569</td>\n",
              "<td>169.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9786187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0742427</td>\n",
              "<td>1.0</td>\n",
              "<td>357.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9786187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4840417</td>\n",
              "<td>0.7924629</td>\n",
              "<td>169.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3209067</td>\n",
              "<td>0.8834244</td>\n",
              "<td>212.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3345247</td>\n",
              "<td>0.8885054</td>\n",
              "<td>205.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9786187</td>\n",
              "<td>549.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9786187</td>\n",
              "<td>341.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0291900</td>\n",
              "<td>549.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0742427</td>\n",
              "<td>342.0</td>\n",
              "<td>357.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9786187</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9786187</td>\n",
              "<td>0.9970760</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0291900</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0742427</td>\n",
              "<td>1.0</td>\n",
              "<td>357.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-17.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-17 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-17 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-17 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-17 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-17 .h2o-table th,\n",
              "#h2o-table-17 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-17 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-17\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 38.38 %, avg score: 38.40 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0101010</td>\n",
              "<td>0.9749711</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9766456</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9766456</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.0263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.0263158</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0202020</td>\n",
              "<td>0.9714981</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9730373</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9748415</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.0526316</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.0526316</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0303030</td>\n",
              "<td>0.9695154</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9705874</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9734234</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.0789474</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.0789474</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0404040</td>\n",
              "<td>0.9677898</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9686473</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9722294</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.1052632</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.1052632</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.0505051</td>\n",
              "<td>0.9663900</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9670991</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9712033</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.1315789</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.1315789</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1010101</td>\n",
              "<td>0.9517049</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9596342</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9654188</td>\n",
              "<td>0.1315789</td>\n",
              "<td>0.2631579</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.2631579</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.1503928</td>\n",
              "<td>0.9297382</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9415044</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9575663</td>\n",
              "<td>0.1286550</td>\n",
              "<td>0.3918129</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.3918129</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2008979</td>\n",
              "<td>0.8514329</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.8893898</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9404270</td>\n",
              "<td>0.1315789</td>\n",
              "<td>0.5233918</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.5233918</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3007856</td>\n",
              "<td>0.6299266</td>\n",
              "<td>2.1954465</td>\n",
              "<td>2.4691673</td>\n",
              "<td>0.8426966</td>\n",
              "<td>0.7413490</td>\n",
              "<td>0.9477612</td>\n",
              "<td>0.8743153</td>\n",
              "<td>0.2192982</td>\n",
              "<td>0.7426901</td>\n",
              "<td>119.5446481</td>\n",
              "<td>146.9167321</td>\n",
              "<td>0.7171891</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4029181</td>\n",
              "<td>0.3311975</td>\n",
              "<td>1.3455755</td>\n",
              "<td>2.1843571</td>\n",
              "<td>0.5164835</td>\n",
              "<td>0.4597418</td>\n",
              "<td>0.8384401</td>\n",
              "<td>0.7692284</td>\n",
              "<td>0.1374269</td>\n",
              "<td>0.8801170</td>\n",
              "<td>34.5575477</td>\n",
              "<td>118.4357132</td>\n",
              "<td>0.7744703</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5005612</td>\n",
              "<td>0.2024575</td>\n",
              "<td>0.5989111</td>\n",
              "<td>1.8750885</td>\n",
              "<td>0.2298851</td>\n",
              "<td>0.2592957</td>\n",
              "<td>0.7197309</td>\n",
              "<td>0.6697572</td>\n",
              "<td>0.0584795</td>\n",
              "<td>0.9385965</td>\n",
              "<td>-40.1088929</td>\n",
              "<td>87.5088506</td>\n",
              "<td>0.7109098</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6004489</td>\n",
              "<td>0.1355478</td>\n",
              "<td>0.3512714</td>\n",
              "<td>1.6215937</td>\n",
              "<td>0.1348315</td>\n",
              "<td>0.1651401</td>\n",
              "<td>0.6224299</td>\n",
              "<td>0.5858116</td>\n",
              "<td>0.0350877</td>\n",
              "<td>0.9736842</td>\n",
              "<td>-64.8728563</td>\n",
              "<td>62.1593704</td>\n",
              "<td>0.6057425</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7003367</td>\n",
              "<td>0.0938176</td>\n",
              "<td>0.1463631</td>\n",
              "<td>1.4111842</td>\n",
              "<td>0.0561798</td>\n",
              "<td>0.1121692</td>\n",
              "<td>0.5416667</td>\n",
              "<td>0.5182568</td>\n",
              "<td>0.0146199</td>\n",
              "<td>0.9883041</td>\n",
              "<td>-85.3636901</td>\n",
              "<td>41.1184211</td>\n",
              "<td>0.4673569</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8002245</td>\n",
              "<td>0.0781551</td>\n",
              "<td>0.0878179</td>\n",
              "<td>1.2459954</td>\n",
              "<td>0.0337079</td>\n",
              "<td>0.0868113</td>\n",
              "<td>0.4782609</td>\n",
              "<td>0.4644017</td>\n",
              "<td>0.0087719</td>\n",
              "<td>0.9970760</td>\n",
              "<td>-91.2182141</td>\n",
              "<td>24.5995423</td>\n",
              "<td>0.3194804</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9001122</td>\n",
              "<td>0.0648662</td>\n",
              "<td>0.0292726</td>\n",
              "<td>1.1109726</td>\n",
              "<td>0.0112360</td>\n",
              "<td>0.0723062</td>\n",
              "<td>0.4264339</td>\n",
              "<td>0.4208899</td>\n",
              "<td>0.0029240</td>\n",
              "<td>1.0</td>\n",
              "<td>-97.0727380</td>\n",
              "<td>11.0972569</td>\n",
              "<td>0.1621129</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0291900</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0516458</td>\n",
              "<td>0.3838384</td>\n",
              "<td>0.3840069</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomial: gbm\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.12564572227434634\n",
              "RMSE: 0.35446540349425687\n",
              "LogLoss: 0.4067842408627829\n",
              "Mean Per-Class Error: 0.1848816029143898\n",
              "AUC: 0.8754673569168824\n",
              "AUCPR: 0.8577731956886376\n",
              "Gini: 0.7509347138337648</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-18.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-18 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-18 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-18 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-18 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-18 .h2o-table th,\n",
              "#h2o-table-18 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-18 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-18\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4002474396191816</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>468.0</td>\n",
              "<td>81.0</td>\n",
              "<td>0.1475</td>\n",
              "<td> (81.0/549.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>76.0</td>\n",
              "<td>266.0</td>\n",
              "<td>0.2222</td>\n",
              "<td> (76.0/342.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>544.0</td>\n",
              "<td>347.0</td>\n",
              "<td>0.1762</td>\n",
              "<td> (157.0/891.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-19.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-19 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-19 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-19 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-19 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-19 .h2o-table th,\n",
              "#h2o-table-19 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-19 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-19\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.4002474</td>\n",
              "<td>0.7721335</td>\n",
              "<td>194.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.1867235</td>\n",
              "<td>0.8205689</td>\n",
              "<td>264.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.6402772</td>\n",
              "<td>0.8210604</td>\n",
              "<td>128.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.5548825</td>\n",
              "<td>0.8395062</td>\n",
              "<td>146.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9960563</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0196822</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9960563</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.5548825</td>\n",
              "<td>0.6565436</td>\n",
              "<td>146.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3458266</td>\n",
              "<td>0.8040936</td>\n",
              "<td>213.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.4002474</td>\n",
              "<td>0.8151184</td>\n",
              "<td>194.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9960563</td>\n",
              "<td>549.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9960563</td>\n",
              "<td>341.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0160240</td>\n",
              "<td>549.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0196822</td>\n",
              "<td>342.0</td>\n",
              "<td>397.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9960563</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9960563</td>\n",
              "<td>0.9970760</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0160240</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0196822</td>\n",
              "<td>1.0</td>\n",
              "<td>397.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-20.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-20 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-20 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-20 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-20 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-20 .h2o-table th,\n",
              "#h2o-table-20 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-20 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-20\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 38.38 %, avg score: 38.23 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0101010</td>\n",
              "<td>0.9796795</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9869037</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9869037</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.0263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.0263158</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0202020</td>\n",
              "<td>0.9681415</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9731575</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9800306</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.0526316</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.0526316</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0303030</td>\n",
              "<td>0.9629498</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9660693</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9753769</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.0789474</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.0789474</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0404040</td>\n",
              "<td>0.9603723</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9615113</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9719105</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.1052632</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.1052632</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.0505051</td>\n",
              "<td>0.9571395</td>\n",
              "<td>2.6052632</td>\n",
              "<td>2.6052632</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9594817</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9694247</td>\n",
              "<td>0.0263158</td>\n",
              "<td>0.1315789</td>\n",
              "<td>160.5263158</td>\n",
              "<td>160.5263158</td>\n",
              "<td>0.1315789</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1010101</td>\n",
              "<td>0.9315320</td>\n",
              "<td>2.4894737</td>\n",
              "<td>2.5473684</td>\n",
              "<td>0.9555556</td>\n",
              "<td>0.9441528</td>\n",
              "<td>0.9777778</td>\n",
              "<td>0.9567888</td>\n",
              "<td>0.1257310</td>\n",
              "<td>0.2573099</td>\n",
              "<td>148.9473684</td>\n",
              "<td>154.7368421</td>\n",
              "<td>0.2536670</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.1503928</td>\n",
              "<td>0.9023035</td>\n",
              "<td>2.25</td>\n",
              "<td>2.4497251</td>\n",
              "<td>0.8636364</td>\n",
              "<td>0.9153417</td>\n",
              "<td>0.9402985</td>\n",
              "<td>0.9431793</td>\n",
              "<td>0.1111111</td>\n",
              "<td>0.3684211</td>\n",
              "<td>125.0</td>\n",
              "<td>144.9725059</td>\n",
              "<td>0.3538491</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2020202</td>\n",
              "<td>0.8125775</td>\n",
              "<td>2.4919908</td>\n",
              "<td>2.4605263</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.8644519</td>\n",
              "<td>0.9444444</td>\n",
              "<td>0.9230601</td>\n",
              "<td>0.1286550</td>\n",
              "<td>0.4970760</td>\n",
              "<td>149.1990847</td>\n",
              "<td>146.0526316</td>\n",
              "<td>0.4788611</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3007856</td>\n",
              "<td>0.5932703</td>\n",
              "<td>1.8059211</td>\n",
              "<td>2.2455813</td>\n",
              "<td>0.6931818</td>\n",
              "<td>0.7076229</td>\n",
              "<td>0.8619403</td>\n",
              "<td>0.8523195</td>\n",
              "<td>0.1783626</td>\n",
              "<td>0.6754386</td>\n",
              "<td>80.5921053</td>\n",
              "<td>124.5581304</td>\n",
              "<td>0.6080433</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4006734</td>\n",
              "<td>0.3810192</td>\n",
              "<td>1.0830869</td>\n",
              "<td>1.9557718</td>\n",
              "<td>0.4157303</td>\n",
              "<td>0.4730918</td>\n",
              "<td>0.7507003</td>\n",
              "<td>0.7577781</td>\n",
              "<td>0.1081871</td>\n",
              "<td>0.7836257</td>\n",
              "<td>8.3086931</td>\n",
              "<td>95.5771782</td>\n",
              "<td>0.6215128</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5005612</td>\n",
              "<td>0.1994466</td>\n",
              "<td>0.8196334</td>\n",
              "<td>1.7290536</td>\n",
              "<td>0.3146067</td>\n",
              "<td>0.2879655</td>\n",
              "<td>0.6636771</td>\n",
              "<td>0.6640263</td>\n",
              "<td>0.0818713</td>\n",
              "<td>0.8654971</td>\n",
              "<td>-18.0366647</td>\n",
              "<td>72.9053576</td>\n",
              "<td>0.5922730</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6004489</td>\n",
              "<td>0.1334150</td>\n",
              "<td>0.3512714</td>\n",
              "<td>1.4998524</td>\n",
              "<td>0.1348315</td>\n",
              "<td>0.1642208</td>\n",
              "<td>0.5757009</td>\n",
              "<td>0.5808811</td>\n",
              "<td>0.0350877</td>\n",
              "<td>0.9005848</td>\n",
              "<td>-64.8728563</td>\n",
              "<td>49.9852435</td>\n",
              "<td>0.4871057</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.7003367</td>\n",
              "<td>0.1062350</td>\n",
              "<td>0.2341810</td>\n",
              "<td>1.3193320</td>\n",
              "<td>0.0898876</td>\n",
              "<td>0.1198342</td>\n",
              "<td>0.5064103</td>\n",
              "<td>0.5151228</td>\n",
              "<td>0.0233918</td>\n",
              "<td>0.9239766</td>\n",
              "<td>-76.5819042</td>\n",
              "<td>31.9331984</td>\n",
              "<td>0.3629566</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8002245</td>\n",
              "<td>0.0852813</td>\n",
              "<td>0.1756357</td>\n",
              "<td>1.1765705</td>\n",
              "<td>0.0674157</td>\n",
              "<td>0.0947699</td>\n",
              "<td>0.4516129</td>\n",
              "<td>0.4626524</td>\n",
              "<td>0.0175439</td>\n",
              "<td>0.9415205</td>\n",
              "<td>-82.4364281</td>\n",
              "<td>17.6570458</td>\n",
              "<td>0.2293165</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.9001122</td>\n",
              "<td>0.0616176</td>\n",
              "<td>0.2927262</td>\n",
              "<td>1.0784880</td>\n",
              "<td>0.1123596</td>\n",
              "<td>0.0731676</td>\n",
              "<td>0.4139651</td>\n",
              "<td>0.4194302</td>\n",
              "<td>0.0292398</td>\n",
              "<td>0.9707602</td>\n",
              "<td>-70.7273802</td>\n",
              "<td>7.8487991</td>\n",
              "<td>0.1146582</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0155573</td>\n",
              "<td>0.2927262</td>\n",
              "<td>1.0</td>\n",
              "<td>0.1123596</td>\n",
              "<td>0.0474298</td>\n",
              "<td>0.3838384</td>\n",
              "<td>0.3822719</td>\n",
              "<td>0.0292398</td>\n",
              "<td>1.0</td>\n",
              "<td>-70.7273802</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-21.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-21 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-21 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-21 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-21 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-21 .h2o-table th,\n",
              "#h2o-table-21 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-21 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-21\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.8439771</td>\n",
              "<td>0.0416259</td>\n",
              "<td>0.8603352</td>\n",
              "<td>0.8651685</td>\n",
              "<td>0.7808989</td>\n",
              "<td>0.8876405</td>\n",
              "<td>0.8258427</td></tr>\n",
              "<tr><td>aic</td>\n",
              "<td>nan</td>\n",
              "<td>0.0</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.8687463</td>\n",
              "<td>0.0366945</td>\n",
              "<td>0.8844064</td>\n",
              "<td>0.8633398</td>\n",
              "<td>0.8116978</td>\n",
              "<td>0.9116640</td>\n",
              "<td>0.8726233</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.1560228</td>\n",
              "<td>0.0416259</td>\n",
              "<td>0.1396648</td>\n",
              "<td>0.1348315</td>\n",
              "<td>0.2191011</td>\n",
              "<td>0.1123596</td>\n",
              "<td>0.1741573</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>27.8</td>\n",
              "<td>7.395945</td>\n",
              "<td>25.0</td>\n",
              "<td>24.0</td>\n",
              "<td>39.0</td>\n",
              "<td>20.0</td>\n",
              "<td>31.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.8088298</td>\n",
              "<td>0.0694661</td>\n",
              "<td>0.8139535</td>\n",
              "<td>0.8737864</td>\n",
              "<td>0.7142857</td>\n",
              "<td>0.875</td>\n",
              "<td>0.7671233</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.7921073</td>\n",
              "<td>0.0485790</td>\n",
              "<td>0.8175182</td>\n",
              "<td>0.8181818</td>\n",
              "<td>0.7111111</td>\n",
              "<td>0.8305085</td>\n",
              "<td>0.7832168</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.7777265</td>\n",
              "<td>0.0432324</td>\n",
              "<td>0.8211144</td>\n",
              "<td>0.7692308</td>\n",
              "<td>0.7079646</td>\n",
              "<td>0.7903226</td>\n",
              "<td>0.8</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.6098633</td>\n",
              "<td>0.1227433</td>\n",
              "<td>2.6323528</td>\n",
              "<td>2.4383562</td>\n",
              "<td>2.6176472</td>\n",
              "<td>2.78125</td>\n",
              "<td>2.5797102</td></tr>\n",
              "<tr><td>loglikelihood</td>\n",
              "<td>nan</td>\n",
              "<td>0.0</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td>\n",
              "<td>nan</td></tr>\n",
              "<tr><td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td></tr>\n",
              "<tr><td>mcc</td>\n",
              "<td>0.670955</td>\n",
              "<td>0.0869331</td>\n",
              "<td>0.7044551</td>\n",
              "<td>0.7231524</td>\n",
              "<td>0.5346942</td>\n",
              "<td>0.7534089</td>\n",
              "<td>0.6390644</td></tr>\n",
              "<tr><td>mean_per_class_accuracy</td>\n",
              "<td>0.8299896</td>\n",
              "<td>0.0381387</td>\n",
              "<td>0.8532062</td>\n",
              "<td>0.8460535</td>\n",
              "<td>0.7665775</td>\n",
              "<td>0.8608827</td>\n",
              "<td>0.8232283</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.1700104</td>\n",
              "<td>0.0381387</td>\n",
              "<td>0.1467939</td>\n",
              "<td>0.1539465</td>\n",
              "<td>0.2334225</td>\n",
              "<td>0.1391173</td>\n",
              "<td>0.1767717</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.1274166</td>\n",
              "<td>0.0260736</td>\n",
              "<td>0.1165862</td>\n",
              "<td>0.1182224</td>\n",
              "<td>0.1697372</td>\n",
              "<td>0.1008104</td>\n",
              "<td>0.1317270</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.8494062</td>\n",
              "<td>0.0582252</td>\n",
              "<td>0.8773087</td>\n",
              "<td>0.8791372</td>\n",
              "<td>0.7500293</td>\n",
              "<td>0.8942694</td>\n",
              "<td>0.8462867</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.8214861</td>\n",
              "<td>0.0887440</td>\n",
              "<td>0.8115942</td>\n",
              "<td>0.9152542</td>\n",
              "<td>0.7164179</td>\n",
              "<td>0.9074074</td>\n",
              "<td>0.7567568</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.4609438</td>\n",
              "<td>0.1088192</td>\n",
              "<td>0.5050956</td>\n",
              "<td>0.5113168</td>\n",
              "<td>0.2810225</td>\n",
              "<td>0.5622155</td>\n",
              "<td>0.4450686</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.7692714</td>\n",
              "<td>0.0490906</td>\n",
              "<td>0.8235294</td>\n",
              "<td>0.739726</td>\n",
              "<td>0.7058824</td>\n",
              "<td>0.765625</td>\n",
              "<td>0.8115942</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.3555445</td>\n",
              "<td>0.0354392</td>\n",
              "<td>0.3414472</td>\n",
              "<td>0.3438348</td>\n",
              "<td>0.4119917</td>\n",
              "<td>0.3175065</td>\n",
              "<td>0.3629422</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.8907078</td>\n",
              "<td>0.0618242</td>\n",
              "<td>0.8828829</td>\n",
              "<td>0.9523810</td>\n",
              "<td>0.8272727</td>\n",
              "<td>0.9561403</td>\n",
              "<td>0.8348624</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "<pre style='font-size: smaller; margin-bottom: 1em;'>[22 rows x 8 columns]</pre></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-22.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-22 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-22 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-22 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-22 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-22 .h2o-table th,\n",
              "#h2o-table-22 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-22 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-22\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Scoring History: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>timestamp</th>\n",
              "<th>duration</th>\n",
              "<th>number_of_trees</th>\n",
              "<th>training_rmse</th>\n",
              "<th>training_logloss</th>\n",
              "<th>training_auc</th>\n",
              "<th>training_pr_auc</th>\n",
              "<th>training_lift</th>\n",
              "<th>training_classification_error</th></tr></thead>\n",
              "    <tbody><tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.372 sec</td>\n",
              "<td>0.0</td>\n",
              "<td>0.4863193</td>\n",
              "<td>0.6659120</td>\n",
              "<td>0.5</td>\n",
              "<td>0.3838384</td>\n",
              "<td>1.0</td>\n",
              "<td>0.6161616</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.399 sec</td>\n",
              "<td>5.0</td>\n",
              "<td>0.3897245</td>\n",
              "<td>0.4856606</td>\n",
              "<td>0.9270630</td>\n",
              "<td>0.9095731</td>\n",
              "<td>2.6052632</td>\n",
              "<td>0.1290685</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.422 sec</td>\n",
              "<td>10.0</td>\n",
              "<td>0.3445253</td>\n",
              "<td>0.4034886</td>\n",
              "<td>0.9338510</td>\n",
              "<td>0.9172270</td>\n",
              "<td>2.6052632</td>\n",
              "<td>0.1268238</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.447 sec</td>\n",
              "<td>15.0</td>\n",
              "<td>0.3183289</td>\n",
              "<td>0.3521452</td>\n",
              "<td>0.9424898</td>\n",
              "<td>0.9265312</td>\n",
              "<td>2.6052632</td>\n",
              "<td>0.1111111</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.470 sec</td>\n",
              "<td>20.0</td>\n",
              "<td>0.3053636</td>\n",
              "<td>0.3242805</td>\n",
              "<td>0.9459517</td>\n",
              "<td>0.9316251</td>\n",
              "<td>2.6052632</td>\n",
              "<td>0.1066218</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.489 sec</td>\n",
              "<td>25.0</td>\n",
              "<td>0.2952650</td>\n",
              "<td>0.3027665</td>\n",
              "<td>0.9493044</td>\n",
              "<td>0.9363860</td>\n",
              "<td>2.6052632</td>\n",
              "<td>0.1032548</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.507 sec</td>\n",
              "<td>30.0</td>\n",
              "<td>0.2862527</td>\n",
              "<td>0.2845291</td>\n",
              "<td>0.9534800</td>\n",
              "<td>0.9416464</td>\n",
              "<td>2.6052632</td>\n",
              "<td>0.1010101</td></tr>\n",
              "<tr><td></td>\n",
              "<td>2025-08-22 14:17:13</td>\n",
              "<td> 1.523 sec</td>\n",
              "<td>33.0</td>\n",
              "<td>0.2820044</td>\n",
              "<td>0.2757159</td>\n",
              "<td>0.9563800</td>\n",
              "<td>0.9448232</td>\n",
              "<td>2.6052632</td>\n",
              "<td>0.0987654</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-23.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-23 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-23 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-23 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-23 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-23 .h2o-table th,\n",
              "#h2o-table-23 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-23 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-23\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Variable Importances: </caption>\n",
              "    <thead><tr><th>variable</th>\n",
              "<th>relative_importance</th>\n",
              "<th>scaled_importance</th>\n",
              "<th>percentage</th></tr></thead>\n",
              "    <tbody><tr><td>adult_male</td>\n",
              "<td>231.2921448</td>\n",
              "<td>1.0</td>\n",
              "<td>0.3203390</td></tr>\n",
              "<tr><td>fare</td>\n",
              "<td>101.3361206</td>\n",
              "<td>0.4381304</td>\n",
              "<td>0.1403502</td></tr>\n",
              "<tr><td>age</td>\n",
              "<td>91.8512268</td>\n",
              "<td>0.3971221</td>\n",
              "<td>0.1272137</td></tr>\n",
              "<tr><td>pclass</td>\n",
              "<td>71.0011749</td>\n",
              "<td>0.3069762</td>\n",
              "<td>0.0983364</td></tr>\n",
              "<tr><td>who</td>\n",
              "<td>62.5803871</td>\n",
              "<td>0.2705686</td>\n",
              "<td>0.0866737</td></tr>\n",
              "<tr><td>sex</td>\n",
              "<td>50.1718216</td>\n",
              "<td>0.2169197</td>\n",
              "<td>0.0694878</td></tr>\n",
              "<tr><td>deck</td>\n",
              "<td>34.4982071</td>\n",
              "<td>0.1491543</td>\n",
              "<td>0.0477799</td></tr>\n",
              "<tr><td>class</td>\n",
              "<td>23.2358856</td>\n",
              "<td>0.1004612</td>\n",
              "<td>0.0321816</td></tr>\n",
              "<tr><td>sibsp</td>\n",
              "<td>21.0053177</td>\n",
              "<td>0.0908173</td>\n",
              "<td>0.0290923</td></tr>\n",
              "<tr><td>parch</td>\n",
              "<td>12.5568810</td>\n",
              "<td>0.0542901</td>\n",
              "<td>0.0173912</td></tr>\n",
              "<tr><td>embark_town</td>\n",
              "<td>10.2540731</td>\n",
              "<td>0.0443339</td>\n",
              "<td>0.0142019</td></tr>\n",
              "<tr><td>embarked</td>\n",
              "<td>7.0957193</td>\n",
              "<td>0.0306786</td>\n",
              "<td>0.0098276</td></tr>\n",
              "<tr><td>alone</td>\n",
              "<td>5.1441374</td>\n",
              "<td>0.0222409</td>\n",
              "<td>0.0071246</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üîπ Model Performance (Training vs CV)\n",
        "\n",
        "### üìå On Training Data\n",
        "\n",
        "* **AUC:** 0.956 ‚Üí excellent discrimination\n",
        "* **Accuracy:** \\~90.2%\n",
        "* **LogLoss:** 0.276 ‚Üí lower is better, means predictions are confident\n",
        "* **Error Rate:** \\~9.8% (only \\~88 misclassifications out of 891)\n",
        "\n",
        "### üìå On Cross-Validation Data\n",
        "\n",
        "* **AUC:** 0.875 ‚Üí still strong (avoids overfitting)\n",
        "* **Accuracy:** \\~84.4%\n",
        "* **LogLoss:** 0.407 ‚Üí reasonable predictive confidence\n",
        "* **Error Rate:** \\~17.6% (slightly higher but expected on unseen folds)\n",
        "\n",
        "‚úÖ This shows the model generalizes well, not just memorizing training data.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Confusion Matrix (CV, best F1 @ threshold \\~0.40)\n",
        "\n",
        "* **True Negatives (0 ‚Üí 0):** 468\n",
        "* **False Positives (0 ‚Üí 1):** 81\n",
        "* **False Negatives (1 ‚Üí 0):** 76\n",
        "* **True Positives (1 ‚Üí 1):** 266\n",
        "\n",
        "üëâ Interpretation:\n",
        "\n",
        "* It catches **most survivors (266)**, but misses 76.\n",
        "* It wrongly predicts 81 non-survivors as survivors.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Feature Importance (Top Predictors)\n",
        "\n",
        "From GBM‚Äôs `varimp` table:\n",
        "\n",
        "1. `adult_male` ‚Üí 32% importance\n",
        "2. `fare` ‚Üí 14%\n",
        "3. `age` ‚Üí 12%\n",
        "4. `pclass` ‚Üí 9.8%\n",
        "5. `who` ‚Üí 8.7%\n",
        "6. `sex` ‚Üí 7%\n",
        "\n",
        "‚ú® Key Insight: Gender + socio-economic status (fare, class, male/female) + age were the **most important factors** for Titanic survival prediction.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vpPelpXJDW3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best model\n",
        "best_model = aml.leader\n",
        "\n",
        "# Evaluate on test data\n",
        "perf = best_model.model_performance(test)\n",
        "\n",
        "print(\"AUC:\", perf.auc())\n",
        "print(\"Accuracy:\", perf.accuracy())\n",
        "print(\"Confusion Matrix:\")\n",
        "print(perf.confusion_matrix()) # Print the confusion matrix directly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mugJZTT7-cy",
        "outputId": "4f2e0988-c0b2-477a-da26-00629281d40a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9673423423423423\n",
            "Accuracy: [[0.5608224593825432, 0.9090909090909091]]\n",
            "Confusion Matrix:\n",
            "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.48404168942857645\n",
            "       0    1    Error    Rate\n",
            "-----  ---  ---  -------  ------------\n",
            "0      106  5    0.045    (5.0/111.0)\n",
            "1      12   64   0.1579   (12.0/76.0)\n",
            "Total  118  69   0.0909   (17.0/187.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üîπ Test Set Performance (Final Check)\n",
        "\n",
        "From your screenshot:\n",
        "\n",
        "* **AUC:** 0.967 ‚Üí outstanding predictive power\n",
        "* **Accuracy:** 90.9%\n",
        "* **Confusion Matrix (at best F1 threshold ‚âà 0.48):**\n",
        "\n",
        "| Actual / Pred   | Pred 0 | Pred 1 | Error | Rate     |\n",
        "| --------------- | ------ | ------ | ----- | -------- |\n",
        "| 0 (non-survive) | 106    | 5      | 0.045 | (5/111)  |\n",
        "| 1 (survive)     | 12     | 64     | 0.157 | (12/76)  |\n",
        "| **Total**       | 118    | 69     | 0.090 | (17/187) |\n",
        "\n",
        "‚úÖ Interpretation:\n",
        "\n",
        "* Only **17 misclassifications out of 187 passengers**.\n",
        "* Model is much better at predicting **non-survivors (95.5% accuracy)** than survivors (\\~84.2%).\n",
        "* Still a very strong generalization performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### Confusion Matrix (Test Data)\n",
        "| Actual / Pred | Pred 0 | Pred 1 | Error | Rate |\n",
        "|---------------|--------|--------|-------|------|\n",
        "| 0 (non-survive) | 106 | 5  | 0.045 | (5/111) |\n",
        "| 1 (survive)    | 12  | 64 | 0.157 | (12/76) |\n",
        "| **Total**      | 118 | 69 | 0.090 | (17/187) |\n",
        "\n",
        "‚úÖ The model generalizes very well: **AUC ~0.97** confirms strong predictive power even on unseen test data.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "BGKh7ZPPG5sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîπ Step 6 ‚Äî Visualize & Interpret Results**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. ‚úÖ Leaderboard**\n",
        "\n",
        "---\n",
        "\n",
        "shows the top-performing models ranked by AUC/Accuracy."
      ],
      "metadata": {
        "id": "Lyop6pII79sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View leaderboard\n",
        "lb = aml.leaderboard #aml.leaderboard ‚Üí Shows all models ranked by accuracy (or AUC, depending on task).\n",
        "lb.head(rows=10) #.head(rows=10) ‚Üí Displays top 10 models.\n",
        "\n",
        "#This leaderboard is the highlight output ‚Äî showing the best-performing models H2O found."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "YHtPMzb1IWa7",
        "outputId": "d32de174-509d-474c-b92e-acf96491c4bc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                                                      auc    logloss     aucpr    mean_per_class_error      rmse       mse\n",
              "-------------------------------------------------------  --------  ---------  --------  ----------------------  --------  --------\n",
              "GBM_3_AutoML_2_20250822_141659                           0.875467   0.406784  0.857773                0.184882  0.354465  0.125646\n",
              "StackedEnsemble_AllModels_1_AutoML_2_20250822_141659     0.875047   0.403938  0.855489                0.18167   0.353077  0.124664\n",
              "GBM_4_AutoML_2_20250822_141659                           0.874927   0.407956  0.859302                0.186032  0.354907  0.125959\n",
              "StackedEnsemble_BestOfFamily_2_AutoML_2_20250822_141659  0.87306    0.407956  0.850538                0.184019  0.355096  0.126093\n",
              "XGBoost_2_AutoML_2_20250822_141659                       0.870392   0.416681  0.843155                0.179777  0.359708  0.12939\n",
              "GBM_5_AutoML_2_20250822_141659                           0.869995   0.446324  0.842685                0.186463  0.371476  0.137994\n",
              "StackedEnsemble_BestOfFamily_1_AutoML_2_20250822_141659  0.869385   0.416764  0.845842                0.185792  0.35904   0.128909\n",
              "XGBoost_1_AutoML_2_20250822_141659                       0.869153   0.418902  0.840676                0.188189  0.35967   0.129362\n",
              "XGBoost_3_AutoML_2_20250822_141659                       0.868607   0.42723   0.8393                  0.19013   0.364042  0.132527\n",
              "GBM_2_AutoML_2_20250822_141659                           0.868035   0.411854  0.852581                0.182149  0.356721  0.12725\n",
              "[10 rows x 7 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                                               </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>GBM_3_AutoML_2_20250822_141659                         </td><td style=\"text-align: right;\">0.875467</td><td style=\"text-align: right;\"> 0.406784</td><td style=\"text-align: right;\">0.857773</td><td style=\"text-align: right;\">              0.184882</td><td style=\"text-align: right;\">0.354465</td><td style=\"text-align: right;\">0.125646</td></tr>\n",
              "<tr><td>StackedEnsemble_AllModels_1_AutoML_2_20250822_141659   </td><td style=\"text-align: right;\">0.875047</td><td style=\"text-align: right;\"> 0.403938</td><td style=\"text-align: right;\">0.855489</td><td style=\"text-align: right;\">              0.18167 </td><td style=\"text-align: right;\">0.353077</td><td style=\"text-align: right;\">0.124664</td></tr>\n",
              "<tr><td>GBM_4_AutoML_2_20250822_141659                         </td><td style=\"text-align: right;\">0.874927</td><td style=\"text-align: right;\"> 0.407956</td><td style=\"text-align: right;\">0.859302</td><td style=\"text-align: right;\">              0.186032</td><td style=\"text-align: right;\">0.354907</td><td style=\"text-align: right;\">0.125959</td></tr>\n",
              "<tr><td>StackedEnsemble_BestOfFamily_2_AutoML_2_20250822_141659</td><td style=\"text-align: right;\">0.87306 </td><td style=\"text-align: right;\"> 0.407956</td><td style=\"text-align: right;\">0.850538</td><td style=\"text-align: right;\">              0.184019</td><td style=\"text-align: right;\">0.355096</td><td style=\"text-align: right;\">0.126093</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_2_20250822_141659                     </td><td style=\"text-align: right;\">0.870392</td><td style=\"text-align: right;\"> 0.416681</td><td style=\"text-align: right;\">0.843155</td><td style=\"text-align: right;\">              0.179777</td><td style=\"text-align: right;\">0.359708</td><td style=\"text-align: right;\">0.12939 </td></tr>\n",
              "<tr><td>GBM_5_AutoML_2_20250822_141659                         </td><td style=\"text-align: right;\">0.869995</td><td style=\"text-align: right;\"> 0.446324</td><td style=\"text-align: right;\">0.842685</td><td style=\"text-align: right;\">              0.186463</td><td style=\"text-align: right;\">0.371476</td><td style=\"text-align: right;\">0.137994</td></tr>\n",
              "<tr><td>StackedEnsemble_BestOfFamily_1_AutoML_2_20250822_141659</td><td style=\"text-align: right;\">0.869385</td><td style=\"text-align: right;\"> 0.416764</td><td style=\"text-align: right;\">0.845842</td><td style=\"text-align: right;\">              0.185792</td><td style=\"text-align: right;\">0.35904 </td><td style=\"text-align: right;\">0.128909</td></tr>\n",
              "<tr><td>XGBoost_1_AutoML_2_20250822_141659                     </td><td style=\"text-align: right;\">0.869153</td><td style=\"text-align: right;\"> 0.418902</td><td style=\"text-align: right;\">0.840676</td><td style=\"text-align: right;\">              0.188189</td><td style=\"text-align: right;\">0.35967 </td><td style=\"text-align: right;\">0.129362</td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_2_20250822_141659                     </td><td style=\"text-align: right;\">0.868607</td><td style=\"text-align: right;\"> 0.42723 </td><td style=\"text-align: right;\">0.8393  </td><td style=\"text-align: right;\">              0.19013 </td><td style=\"text-align: right;\">0.364042</td><td style=\"text-align: right;\">0.132527</td></tr>\n",
              "<tr><td>GBM_2_AutoML_2_20250822_141659                         </td><td style=\"text-align: right;\">0.868035</td><td style=\"text-align: right;\"> 0.411854</td><td style=\"text-align: right;\">0.852581</td><td style=\"text-align: right;\">              0.182149</td><td style=\"text-align: right;\">0.356721</td><td style=\"text-align: right;\">0.12725 </td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 7 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### üèÜ AutoML Leaderboard (Top 5 Models)\n",
        "\n",
        "| Rank | Model                            | AUC   | LogLoss | Accuracy (approx) |\n",
        "| ---- | -------------------------------- | ----- | ------- | ----------------- |\n",
        "| 1    | **GBM\\_3\\_AutoML**               | 0.875 | 0.407   | \\~83‚Äì84%          |\n",
        "| 2    | StackedEnsemble\\_AllModels\\_1    | 0.875 | 0.404   | \\~83%             |\n",
        "| 3    | GBM\\_4\\_AutoML                   | 0.875 | 0.408   | \\~83%             |\n",
        "| 4    | StackedEnsemble\\_BestOfFamily\\_2 | 0.873 | 0.408   | \\~82‚Äì83%          |\n",
        "| 5    | XGBoost\\_2\\_AutoML               | 0.870 | 0.417   | \\~82%             |\n",
        "\n",
        "‚úÖ The leaderboard shows **GBM models and stacked ensembles dominate** the top positions, with AUCs in the **0.87‚Äì0.88** range.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "iRmxoXUbFWc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. üìä Feature Importance**\n",
        "\n",
        "---\n",
        "\n",
        "Not all models have feature importance (e.g., Stacked Ensembles don‚Äôt).\n",
        "But models like **GBM, XGBoost, Random Forest** do.\n",
        "\n",
        "üëâ Example: Check feature importance for a **GBM model**:\n",
        "---\n",
        "\n",
        "### üîç Interpretation\n",
        "\n",
        "* Feature importance tells you **which input features influenced predictions the most**.\n",
        "* For Titanic dataset, usually:\n",
        "\n",
        "  * `Sex` (Male/Female)\n",
        "  * `Pclass` (Passenger class)\n",
        "  * `Age`\n",
        "  * `Fare`\n",
        "    are top predictors of survival.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KpQ5vqXpH7_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert model_id column into Python list\n",
        "model_ids = lb[\"model_id\"].as_data_frame()[\"model_id\"].tolist()\n",
        "\n",
        "#lb[\"model_id\"] ‚Üí still inside H2OFrame format\n",
        "#.as_data_frame() ‚Üí converts it into a Pandas DataFrame\n",
        "#[\"model_id\"].tolist() ‚Üí gets a plain Python list of strings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNESyqv1MfYP",
        "outputId": "0251bdb9-e8ac-4cb5-decb-c889e58f87ee"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a GBM model from leaderboard\n",
        "gbm_model = h2o.get_model([mid for mid in model_ids if \"GBM\" in mid][0])"
      ],
      "metadata": {
        "id": "8KOmZKwB8J0n"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step by step explanation:**\n",
        "\n",
        "1. **model_ids**\n",
        "\n",
        "   * `'model_ids'` = column that stores IDs of all trained models. this is the plane list of strings we converted earlier.\n",
        "   * Example: `\"GBM_1_AutoML_4_20250821_140331\"`, `\"XGBoost_2_AutoML_...\"`.\n",
        "\n",
        "---\n",
        "\n",
        "2. **`[mid for mid in model_ids if \"GBM\" in mid]`**\n",
        "\n",
        "   * This is a **list comprehension** in Python.\n",
        "   * It loops through each `mid` (model ID) in `model_ids`.\n",
        "   * **Condition:** `if \"GBM\" in mid` ‚Üí Only keep model IDs that contain the text `\"GBM\"`.\n",
        "   * Example result: `[\"GBM_1_AutoML_4_20250821_140331\", \"GBM_2_AutoML_...\"]`.\n",
        "\n",
        "---\n",
        "\n",
        "3. **`[0]`**\n",
        "\n",
        "   * Takes the **first item** from that list of GBM models.\n",
        "   * So if there are multiple GBM models, it just picks the top one.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "MKNAvCoXKN4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show feature importance\n",
        "gbm_model.varimp_plot()\n",
        "\n",
        "#h2o.get_model(...)\n",
        "#This function retrieves the full trained model object from H2O, using its model_id.\n",
        "#Without this, you just have the ID string, not the actual model."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "gAs0wl55KYa-",
        "outputId": "f4342ad2-e5d2-431d-bda0-e20d51db67ce"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<h2o.plot._plot_result._MObject at 0x79dcd6738b00>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### üîé What this plot means\n",
        "\n",
        "These bars show which features were **most useful for predicting survival** on Titanic üö¢:\n",
        "\n",
        "1. **adult\\_male** ‚Üí üöπ Being an adult male strongly reduced survival chances (social norms favored women & children).\n",
        "2. **fare** ‚Üí üí∞ Higher fare = wealthier passengers = often higher survival (first-class lifeboat access).\n",
        "3. **age** ‚Üí üë∂ Younger passengers (especially children) had higher survival.\n",
        "4. **pclass** ‚Üí üéüÔ∏è Ticket class (1st > 2nd > 3rd) mattered a lot.\n",
        "5. **who** ‚Üí (man, woman, child) reinforced survival patterns.\n",
        "6. **sex** ‚Üí Similar to `adult_male`, gender influenced survival.\n",
        "7. **deck** ‚Üí Higher decks (closer to lifeboats) often had higher survival.\n",
        "8. **class** ‚Üí Another representation of `pclass`.\n",
        "9. **sibsp** ‚Üí Number of siblings/spouses aboard (larger families sometimes reduced survival).\n",
        "10. **parch** ‚Üí Number of parents/children aboard.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "awEbFHbpMfEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîπ Step 7 - Export Leaderboard & Feature Importance**"
      ],
      "metadata": {
        "id": "DSIpKiNvNksk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save leaderboard as csv\n",
        "lb.as_data_frame().to_csv(\"leaderboard.csv\", index=False)\n",
        "\n",
        "# Save feature importance for GBM\n",
        "varimp = gbm_model.varimp(use_pandas=True)\n",
        "varimp.to_csv(\"feature_importance.csv\", index=False)"
      ],
      "metadata": {
        "id": "yY7oR9rbLnuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8851f4-85af-4b44-e2d2-24bc3d10b4df"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5-62kVoOibc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}